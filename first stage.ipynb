{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5577c4f2-cba7-44e7-8bd4-609cda91ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d688c1-4de4-4b8b-872b-c9e464cf7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('subset_10_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57b6bca-b0e6-4d31-bbe2-95261e2d3dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>collection</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>github_open_source_10_2_0</td>\n",
       "      <td>Github OpenSource</td>\n",
       "      <td>Various open source</td>\n",
       "      <td>from typing import TypeVar\\nfrom ndb_adapter.n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US-3607956D-A_1</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Process for preparing allyl chloride and metha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github_open_source_10_2_1</td>\n",
       "      <td>Github OpenSource</td>\n",
       "      <td>Various open source</td>\n",
       "      <td>﻿namespace NServiceBus.Unicast.Tests\\n{\\n    u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>github_open_source_10_2_2</td>\n",
       "      <td>Github OpenSource</td>\n",
       "      <td>Various open source</td>\n",
       "      <td>// Copyright 2017 The Fuchsia Authors. All rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.go4expert.com/forums/final-year-pro...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>hello people...how are u....i am a student of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51465</th>\n",
       "      <td>bpt6k75250333_2</td>\n",
       "      <td>French-PD-Newspapers</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Nos voisins de l'Est peuvent, en la circonstan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51466</th>\n",
       "      <td>https://openalex.org/W2582118333_1</td>\n",
       "      <td>Spanish-Science-Pile</td>\n",
       "      <td>Various open science</td>\n",
       "      <td>Cuad. Invest. Filol., 42 (2016), 67-80. DOI: 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51467</th>\n",
       "      <td>https://bibtex.github.io/person/Yongseok_Oh.html</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Travelled to:\\n1 × Spain\\nCollaborated with:\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51468</th>\n",
       "      <td>US-78283607-A_3</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Plasmid pLR186 was derived from the commercial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51469</th>\n",
       "      <td>248113_1</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>CC-By-SA</td>\n",
       "      <td>Visão Geral\\n\\nPROFIBUS (acrónimo de Process F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51470 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              identifier  \\\n",
       "0                              github_open_source_10_2_0   \n",
       "1                                        US-3607956D-A_1   \n",
       "2                              github_open_source_10_2_1   \n",
       "3                              github_open_source_10_2_2   \n",
       "4      http://www.go4expert.com/forums/final-year-pro...   \n",
       "...                                                  ...   \n",
       "51465                                    bpt6k75250333_2   \n",
       "51466                 https://openalex.org/W2582118333_1   \n",
       "51467   https://bibtex.github.io/person/Yongseok_Oh.html   \n",
       "51468                                    US-78283607-A_3   \n",
       "51469                                           248113_1   \n",
       "\n",
       "                          collection                license  \\\n",
       "0                  Github OpenSource    Various open source   \n",
       "1                              USPTO          Public Domain   \n",
       "2                  Github OpenSource    Various open source   \n",
       "3                  Github OpenSource    Various open source   \n",
       "4      Creative Commons Common Crawl  Various open licenses   \n",
       "...                              ...                    ...   \n",
       "51465           French-PD-Newspapers          Public Domain   \n",
       "51466           Spanish-Science-Pile   Various open science   \n",
       "51467  Creative Commons Common Crawl  Various open licenses   \n",
       "51468                          USPTO          Public Domain   \n",
       "51469                      Wikipedia               CC-By-SA   \n",
       "\n",
       "                                                    text  \n",
       "0      from typing import TypeVar\\nfrom ndb_adapter.n...  \n",
       "1      Process for preparing allyl chloride and metha...  \n",
       "2      ﻿namespace NServiceBus.Unicast.Tests\\n{\\n    u...  \n",
       "3      // Copyright 2017 The Fuchsia Authors. All rig...  \n",
       "4      hello people...how are u....i am a student of ...  \n",
       "...                                                  ...  \n",
       "51465  Nos voisins de l'Est peuvent, en la circonstan...  \n",
       "51466  Cuad. Invest. Filol., 42 (2016), 67-80. DOI: 1...  \n",
       "51467  Travelled to:\\n1 × Spain\\nCollaborated with:\\n...  \n",
       "51468  Plasmid pLR186 was derived from the commercial...  \n",
       "51469  Visão Geral\\n\\nPROFIBUS (acrónimo de Process F...  \n",
       "\n",
       "[51470 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0766ec8-cb23-45c7-980d-92da35c9fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langdetect import detect  # for language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ed415b-ae9a-4ec9-b3c8-4c2349ce24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    # 1. Remove programming language entries (Github entries)\n",
    "    df = df[df['collection'] != 'Github OpenSource'].copy()\n",
    "    \n",
    "    # 2. Clean text content\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[\\\\/\\*\\n\\r\\t]+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # 3. Detect English language\n",
    "    def is_english(text):\n",
    "        try:\n",
    "            if pd.isna(text):\n",
    "                return False\n",
    "            return detect(text) == 'en'\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # Add English language flag\n",
    "    df['is_english'] = df['text'].apply(is_english)\n",
    "    \n",
    "    # 4. Filter only English content\n",
    "    df_clean = df[df['is_english']].copy()\n",
    "    \n",
    "    # 5. Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337ae087-f507-4e26-8db0-84127ce041cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('subset_10_2.parquet')  # Replace with your actual file path\n",
    "cleaned_df = clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2a6d34-b7da-4424-857c-14abf7c2e232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>collection</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>is_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-3607956D-A_1</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Process for preparing allyl chloride and metha...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.go4expert.com/forums/final-year-pro...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>hello people...how are u....i am a student of ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sn84022149_1878-07-25_1_2_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>The Daily Leader. rmmm daily, xxcxrr Monday, b...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sermons01blai_1_9</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>The defire of it difeovers a liberal mind, and...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://eunis.eea.europa.eu/species/224803</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Kingdom: Animalia &gt; Phylum: Arthropoda &gt; Class...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21145</th>\n",
       "      <td>https://blog.geoactivegroup.com/2018/11/connec...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Technology | Media | Telecommunications Tuesda...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21146</th>\n",
       "      <td>1108673_2003_2</td>\n",
       "      <td>SEC</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Gain on Sale of Discontinued Operations. For t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21147</th>\n",
       "      <td>collegeofpharmac1959coll_1</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21148</th>\n",
       "      <td>https://bibtex.github.io/person/Yongseok_Oh.html</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Travelled to: 1 × Spain Collaborated with: E.L...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21149</th>\n",
       "      <td>US-78283607-A_3</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Plasmid pLR186 was derived from the commercial...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              identifier  \\\n",
       "0                                        US-3607956D-A_1   \n",
       "1      http://www.go4expert.com/forums/final-year-pro...   \n",
       "2                            sn84022149_1878-07-25_1_2_1   \n",
       "3                                      sermons01blai_1_9   \n",
       "4              http://eunis.eea.europa.eu/species/224803   \n",
       "...                                                  ...   \n",
       "21145  https://blog.geoactivegroup.com/2018/11/connec...   \n",
       "21146                                     1108673_2003_2   \n",
       "21147                         collegeofpharmac1959coll_1   \n",
       "21148   https://bibtex.github.io/person/Yongseok_Oh.html   \n",
       "21149                                    US-78283607-A_3   \n",
       "\n",
       "                          collection                license  \\\n",
       "0                              USPTO          Public Domain   \n",
       "1      Creative Commons Common Crawl  Various open licenses   \n",
       "2                   US-PD-Newspapers          Public Domain   \n",
       "3                         English-PD          Public Domain   \n",
       "4      Creative Commons Common Crawl  Various open licenses   \n",
       "...                              ...                    ...   \n",
       "21145  Creative Commons Common Crawl  Various open licenses   \n",
       "21146                            SEC          Public Domain   \n",
       "21147                     English-PD          Public Domain   \n",
       "21148  Creative Commons Common Crawl  Various open licenses   \n",
       "21149                          USPTO          Public Domain   \n",
       "\n",
       "                                                    text  is_english  \n",
       "0      Process for preparing allyl chloride and metha...        True  \n",
       "1      hello people...how are u....i am a student of ...        True  \n",
       "2      The Daily Leader. rmmm daily, xxcxrr Monday, b...        True  \n",
       "3      The defire of it difeovers a liberal mind, and...        True  \n",
       "4      Kingdom: Animalia > Phylum: Arthropoda > Class...        True  \n",
       "...                                                  ...         ...  \n",
       "21145  Technology | Media | Telecommunications Tuesda...        True  \n",
       "21146  Gain on Sale of Discontinued Operations. For t...        True  \n",
       "21147  COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...        True  \n",
       "21148  Travelled to: 1 × Spain Collaborated with: E.L...        True  \n",
       "21149  Plasmid pLR186 was derived from the commercial...        True  \n",
       "\n",
       "[21150 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ba1f5e-bbba-46b5-8b40-9a17c7aedb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\umair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\umair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\umair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\umair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\umair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f1c84d-5785-4f40-bfd7-a2ff8d1126ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "{'total_rows': 20626, 'avg_text_length': 11962.09934063803, 'content_type_distribution': content_type\n",
      "general       18599\n",
      "short_form     2027\n",
      "Name: count, dtype: int64, 'source_distribution': collection\n",
      "Creative Commons Common Crawl    9937\n",
      "USPTO                            2857\n",
      "US-PD-Newspapers                 2015\n",
      "English-PD                       1704\n",
      "courtlistener                    1518\n",
      "Name: count, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from textstat import textstat\n",
    "\n",
    "def further_clean_dataset(cleaned_df):\n",
    "    # Start with your already cleaned DataFrame that has 'is_english' column\n",
    "    df = cleaned_df.copy()\n",
    "    \n",
    "    # 1. Remove very short texts\n",
    "    min_chars = 100\n",
    "    df = df[df['text'].str.len() > min_chars]\n",
    "    \n",
    "    # 2. Enhanced text cleaning\n",
    "    def deep_clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "            \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep periods and basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?-]', ' ', text)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove multiple periods\n",
    "        text = re.sub(r'\\.{2,}', '.', text)\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "       \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        return ' '.join(words).strip()\n",
    "    \n",
    "    # Apply further cleaning to the already cleaned text\n",
    "    df['text'] = df['text'].apply(deep_clean_text)\n",
    "    \n",
    "    # 3. Quality filters\n",
    "    def text_quality_check(text):\n",
    "        if pd.isna(text):\n",
    "            return False\n",
    "            \n",
    "        # Check number of sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) < 2:  # Require at least 2 sentences\n",
    "            return False\n",
    "            \n",
    "        # Check for reasonable sentence length\n",
    "        avg_sent_len = sum(len(s.split()) for s in sentences) / len(sentences)\n",
    "        if avg_sent_len < 5 or avg_sent_len > 100:  # Filter out very short/long sentences\n",
    "            return False\n",
    "            \n",
    "        # Check text-to-word ratio (detect garbage text)\n",
    "        words = text.split()\n",
    "        if len(words) == 0:\n",
    "            return False\n",
    "        char_to_word_ratio = len(text) / len(words)\n",
    "        if char_to_word_ratio > 20:  # Likely garbage text\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    df['is_quality'] = df['text'].apply(text_quality_check)\n",
    "    \n",
    "    # 4. Source-based filtering\n",
    "    quality_sources = ['Wikipedia', 'courtlistener', 'Caselaw_Access_Project']\n",
    "    df['is_quality_source'] = df['collection'].isin(quality_sources)\n",
    "    \n",
    "    # 5. Create content type tags\n",
    "    def get_content_type(text, source):\n",
    "        if pd.isna(text):\n",
    "            return 'unknown'\n",
    "        \n",
    "        if 'OPINION AND ORDER' in text or 'NOTICE:' in text:\n",
    "            return 'legal'\n",
    "        elif len(text.split('\\n')) > 10:\n",
    "            return 'article'\n",
    "        elif len(text) < 500:\n",
    "            return 'short_form'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    df['content_type'] = df.apply(lambda x: get_content_type(x['text'], x['collection']), axis=1)\n",
    "    \n",
    "    # 6. Final filtering\n",
    "    final_df = df[\n",
    "        (df['is_quality']) & \n",
    "        (df['content_type'] != 'unknown')\n",
    "    ].copy()\n",
    "    \n",
    "    # 7. Drop intermediate columns and reset index\n",
    "    final_df = final_df.drop(['is_quality', 'is_quality_source'], axis=1)\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Usage with your existing cleaned dataset:\n",
    "further_cleaned_df = further_clean_dataset(cleaned_df)\n",
    "\n",
    "# Get statistics\n",
    "def get_dataset_stats(df):\n",
    "    stats = {\n",
    "        'total_rows': len(df),\n",
    "        'avg_text_length': df['text'].str.len().mean(),\n",
    "        'content_type_distribution': df['content_type'].value_counts(),\n",
    "        'source_distribution': df['collection'].value_counts().head()\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Print statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(get_dataset_stats(further_cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aaa485c-7dab-4310-9b05-54de4e1da9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>collection</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>is_english</th>\n",
       "      <th>content_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-3607956D-A_1</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Process preparing allyl chloride methallyl chl...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sn84022149_1878-07-25_1_2_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Daily Leader . rmmm daily , xxcxrr Monday , Tn...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sermons01blai_1_9</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>defire difeovers liberal mind , con- netted ma...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://eunis.eea.europa.eu/species/224803</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Kingdom Animalia Phylum Arthropoda Class Arach...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.perseus.tufts.edu/hopper/text?doc=u...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>23 faith , Moses , born , hidden three month p...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20621</th>\n",
       "      <td>https://blog.geoactivegroup.com/2018/11/connec...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Technology Media Telecommunications Tuesday , ...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20622</th>\n",
       "      <td>1108673_2003_2</td>\n",
       "      <td>SEC</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Gain Sale Discontinued Operations . year ended...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20623</th>\n",
       "      <td>collegeofpharmac1959coll_1</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20624</th>\n",
       "      <td>https://bibtex.github.io/person/Yongseok_Oh.html</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Various open licenses</td>\n",
       "      <td>Travelled 1 Spain Collaborated E.Lee D.Lee Tal...</td>\n",
       "      <td>True</td>\n",
       "      <td>short_form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20625</th>\n",
       "      <td>US-78283607-A_3</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Public Domain</td>\n",
       "      <td>Plasmid pLR186 derived commercially available ...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20626 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              identifier  \\\n",
       "0                                        US-3607956D-A_1   \n",
       "1                            sn84022149_1878-07-25_1_2_1   \n",
       "2                                      sermons01blai_1_9   \n",
       "3              http://eunis.eea.europa.eu/species/224803   \n",
       "4      http://www.perseus.tufts.edu/hopper/text?doc=u...   \n",
       "...                                                  ...   \n",
       "20621  https://blog.geoactivegroup.com/2018/11/connec...   \n",
       "20622                                     1108673_2003_2   \n",
       "20623                         collegeofpharmac1959coll_1   \n",
       "20624   https://bibtex.github.io/person/Yongseok_Oh.html   \n",
       "20625                                    US-78283607-A_3   \n",
       "\n",
       "                          collection                license  \\\n",
       "0                              USPTO          Public Domain   \n",
       "1                   US-PD-Newspapers          Public Domain   \n",
       "2                         English-PD          Public Domain   \n",
       "3      Creative Commons Common Crawl  Various open licenses   \n",
       "4      Creative Commons Common Crawl  Various open licenses   \n",
       "...                              ...                    ...   \n",
       "20621  Creative Commons Common Crawl  Various open licenses   \n",
       "20622                            SEC          Public Domain   \n",
       "20623                     English-PD          Public Domain   \n",
       "20624  Creative Commons Common Crawl  Various open licenses   \n",
       "20625                          USPTO          Public Domain   \n",
       "\n",
       "                                                    text  is_english  \\\n",
       "0      Process preparing allyl chloride methallyl chl...        True   \n",
       "1      Daily Leader . rmmm daily , xxcxrr Monday , Tn...        True   \n",
       "2      defire difeovers liberal mind , con- netted ma...        True   \n",
       "3      Kingdom Animalia Phylum Arthropoda Class Arach...        True   \n",
       "4      23 faith , Moses , born , hidden three month p...        True   \n",
       "...                                                  ...         ...   \n",
       "20621  Technology Media Telecommunications Tuesday , ...        True   \n",
       "20622  Gain Sale Discontinued Operations . year ended...        True   \n",
       "20623  COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...        True   \n",
       "20624  Travelled 1 Spain Collaborated E.Lee D.Lee Tal...        True   \n",
       "20625  Plasmid pLR186 derived commercially available ...        True   \n",
       "\n",
       "      content_type  \n",
       "0          general  \n",
       "1          general  \n",
       "2          general  \n",
       "3          general  \n",
       "4          general  \n",
       "...            ...  \n",
       "20621      general  \n",
       "20622      general  \n",
       "20623      general  \n",
       "20624   short_form  \n",
       "20625      general  \n",
       "\n",
       "[20626 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "further_cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0bb63b-7019-4d4a-961c-9055bdbba835",
   "metadata": {},
   "outputs": [],
   "source": [
    "further_cleaned_df = further_cleaned_df.drop(columns=['license'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe593d1-c87d-4357-bb11-b60527b334b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate entries found: 2094\n",
      "Dataset Statistics after Removing Duplicates:\n",
      "{'total_rows': 19147, 'avg_text_length': 12679.504360996501, 'content_type_distribution': content_type\n",
      "general       17365\n",
      "short_form     1782\n",
      "Name: count, dtype: int64, 'source_distribution': collection\n",
      "Creative Commons Common Crawl    8458\n",
      "USPTO                            2857\n",
      "US-PD-Newspapers                 2015\n",
      "English-PD                       1704\n",
      "courtlistener                    1518\n",
      "Name: count, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Assuming further_cleaned_df is your DataFrame after removing the 'license' column\n",
    "\n",
    "# Identify and remove duplicates based on the 'identifier' column\n",
    "def remove_duplicates(df):\n",
    "    # Check for duplicates based on the 'identifier' column\n",
    "    duplicates = df[df.duplicated(subset='identifier', keep=False)]\n",
    "\n",
    "    # Print the number of duplicates found\n",
    "    print(f\"Number of duplicate entries found: {len(duplicates)}\")\n",
    "\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df_no_duplicates = df.drop_duplicates(subset='identifier', keep='first')\n",
    "\n",
    "    return df_no_duplicates\n",
    "\n",
    "# Remove duplicates from the further_cleaned_df DataFrame\n",
    "further_cleaned_df = remove_duplicates(further_cleaned_df)\n",
    "\n",
    "# Get statistics after removing duplicates\n",
    "def get_dataset_stats(df):\n",
    "    stats = {\n",
    "        'total_rows': len(df),\n",
    "        'avg_text_length': df['text'].str.len().mean(),\n",
    "        'content_type_distribution': df['content_type'].value_counts(),\n",
    "        'source_distribution': df['collection'].value_counts().head()\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Print statistics\n",
    "print(\"Dataset Statistics after Removing Duplicates:\")\n",
    "print(get_dataset_stats(further_cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "686d383c-7fdc-4c59-a15e-aefbfc8e3b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>collection</th>\n",
       "      <th>text</th>\n",
       "      <th>is_english</th>\n",
       "      <th>content_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-3607956D-A_1</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Process preparing allyl chloride methallyl chl...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sn84022149_1878-07-25_1_2_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>Daily Leader . rmmm daily , xxcxrr Monday , Tn...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sermons01blai_1_9</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>defire difeovers liberal mind , con- netted ma...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://eunis.eea.europa.eu/species/224803</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Kingdom Animalia Phylum Arthropoda Class Arach...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.perseus.tufts.edu/hopper/text?doc=u...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>23 faith , Moses , born , hidden three month p...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sn84026089_1876-04-19_1_1_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>I. M. SEVERNS , M.D. , WHOLESALE RETAIL DRUGGI...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://ccforum.com/content/16/6/245</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Review Year review 2011 Critical Care - neuroc...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sn86076142_1909-04-24_1_1_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>Try WEATHER top Daily Bonanza Judicious Advert...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sn84038306_1920-07-14_1_2_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>COLESBURG Chautauqua Colesburg consist five nu...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sn87093029_1908-11-27_1_5_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>TIME great thing always punctual , mean Dollar...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          identifier  \\\n",
       "0                                    US-3607956D-A_1   \n",
       "1                        sn84022149_1878-07-25_1_2_1   \n",
       "2                                  sermons01blai_1_9   \n",
       "3          http://eunis.eea.europa.eu/species/224803   \n",
       "4  http://www.perseus.tufts.edu/hopper/text?doc=u...   \n",
       "5                        sn84026089_1876-04-19_1_1_1   \n",
       "6                http://ccforum.com/content/16/6/245   \n",
       "7                        sn86076142_1909-04-24_1_1_1   \n",
       "8                        sn84038306_1920-07-14_1_2_1   \n",
       "9                        sn87093029_1908-11-27_1_5_1   \n",
       "\n",
       "                      collection  \\\n",
       "0                          USPTO   \n",
       "1               US-PD-Newspapers   \n",
       "2                     English-PD   \n",
       "3  Creative Commons Common Crawl   \n",
       "4  Creative Commons Common Crawl   \n",
       "5               US-PD-Newspapers   \n",
       "6  Creative Commons Common Crawl   \n",
       "7               US-PD-Newspapers   \n",
       "8               US-PD-Newspapers   \n",
       "9               US-PD-Newspapers   \n",
       "\n",
       "                                                text  is_english content_type  \n",
       "0  Process preparing allyl chloride methallyl chl...        True      general  \n",
       "1  Daily Leader . rmmm daily , xxcxrr Monday , Tn...        True      general  \n",
       "2  defire difeovers liberal mind , con- netted ma...        True      general  \n",
       "3  Kingdom Animalia Phylum Arthropoda Class Arach...        True      general  \n",
       "4  23 faith , Moses , born , hidden three month p...        True      general  \n",
       "5  I. M. SEVERNS , M.D. , WHOLESALE RETAIL DRUGGI...        True      general  \n",
       "6  Review Year review 2011 Critical Care - neuroc...        True      general  \n",
       "7  Try WEATHER top Daily Bonanza Judicious Advert...        True      general  \n",
       "8  COLESBURG Chautauqua Colesburg consist five nu...        True      general  \n",
       "9  TIME great thing always punctual , mean Dollar...        True      general  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the final DataFrame\n",
    "further_cleaned_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07d82a2-c3be-4cae-9dfd-c7a8adac209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Statistics:\n",
      "\n",
      "main_category_distribution:\n",
      "main_category\n",
      "legal       9398\n",
      "general     3032\n",
      "science     2985\n",
      "history     2033\n",
      "news         930\n",
      "academic     769\n",
      "Name: count, dtype: int64\n",
      "\n",
      "subcategory_distribution:\n",
      "subcategory\n",
      "court_decision       7882\n",
      "general              3032\n",
      "legal_general         939\n",
      "technology            887\n",
      "history_general       839\n",
      "medical               829\n",
      "science_general       668\n",
      "military_history      509\n",
      "legal_document        458\n",
      "news_general          385\n",
      "thesis                365\n",
      "current_events        323\n",
      "social_history        290\n",
      "political_history     257\n",
      "biology               253\n",
      "research_paper        235\n",
      "environmental         196\n",
      "press_release         155\n",
      "physics               152\n",
      "economic_history      138\n",
      "legal_analysis        119\n",
      "academic_general       92\n",
      "review                 77\n",
      "news_article           67\n",
      "Name: count, dtype: int64\n",
      "\n",
      "avg_text_length_by_category:\n",
      "main_category\n",
      "academic     7049.574772\n",
      "general      2609.011214\n",
      "history     10663.020167\n",
      "legal       17005.579804\n",
      "news         4161.243011\n",
      "science     14766.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "avg_sentences_by_category:\n",
      "main_category\n",
      "academic     89.736021\n",
      "general      28.975594\n",
      "history     145.414658\n",
      "legal       210.478932\n",
      "news         56.178495\n",
      "science     156.105528\n",
      "Name: sentence_count, dtype: float64\n",
      "\n",
      "source_category_correlation:\n",
      "main_category                  academic  general  history  legal  news  \\\n",
      "collection                                                               \n",
      "Caselaw_Access_Project                0        0        0    524     0   \n",
      "Creative Commons Common Crawl       637     2228     1234   1611   760   \n",
      "Dutch-PD                              0        4        1      3     0   \n",
      "English-PD                           15       12      301   1308    13   \n",
      "Eurlex                                2        4        0     80     1   \n",
      "European Open Data                    0        0        3     22     1   \n",
      "Eurovoc                               0        2        1     12     0   \n",
      "French Open Data                      2        1        5     70     7   \n",
      "French-PD-Newspapers                  0        0        1      0     0   \n",
      "French-PD-diverse                     0        0        0      9     0   \n",
      "French-Science-Pile                  13        1        4     14     0   \n",
      "GATT_library                          0       10        6     20     3   \n",
      "German-PD                             0        0        0      1     0   \n",
      "German-Science-Pile                   3        0        2      0     0   \n",
      "Latin-PD                              0        1        3      1     0   \n",
      "LoC-PD-Books                          1        2       20     54     0   \n",
      "Multilingual-PD                       0        0        0      2     0   \n",
      "NewZealand-PD-Newspapers              0        3        8    167     3   \n",
      "OECD                                  0        1        0      2     0   \n",
      "Open-Science-Pile                     8        1        3      5     0   \n",
      "Portuguese-PD                         0        0        0      2     0   \n",
      "SEC                                   0        0        4    116     1   \n",
      "Spanish-PD-Books                      0        0        1      0     0   \n",
      "Spanish-Science-Pile                  0        0        1      0     0   \n",
      "US-PD-Books                           5        5      115    408     1   \n",
      "US-PD-Newspapers                     32       78      171   1604    93   \n",
      "USPTO                                19      434       69   1749    32   \n",
      "WTO                                   0       10        2     15     0   \n",
      "Wikipedia                            31      210       70     33     7   \n",
      "Youtube-Commons-Whisper               1       25        8     48     8   \n",
      "courtlistener                         0        0        0   1518     0   \n",
      "\n",
      "main_category                  science  \n",
      "collection                              \n",
      "Caselaw_Access_Project               0  \n",
      "Creative Commons Common Crawl     1988  \n",
      "Dutch-PD                             0  \n",
      "English-PD                          55  \n",
      "Eurlex                               4  \n",
      "European Open Data                   5  \n",
      "Eurovoc                              8  \n",
      "French Open Data                    11  \n",
      "French-PD-Newspapers                 0  \n",
      "French-PD-diverse                    2  \n",
      "French-Science-Pile                158  \n",
      "GATT_library                         5  \n",
      "German-PD                            0  \n",
      "German-Science-Pile                  1  \n",
      "Latin-PD                             2  \n",
      "LoC-PD-Books                         2  \n",
      "Multilingual-PD                      0  \n",
      "NewZealand-PD-Newspapers             3  \n",
      "OECD                                 1  \n",
      "Open-Science-Pile                   48  \n",
      "Portuguese-PD                        0  \n",
      "SEC                                 26  \n",
      "Spanish-PD-Books                     0  \n",
      "Spanish-Science-Pile                 8  \n",
      "US-PD-Books                         18  \n",
      "US-PD-Newspapers                    37  \n",
      "USPTO                              554  \n",
      "WTO                                  3  \n",
      "Wikipedia                           33  \n",
      "Youtube-Commons-Whisper             13  \n",
      "courtlistener                        0  \n"
     ]
    }
   ],
   "source": [
    "def get_detailed_content_type(text, source):\n",
    "    \"\"\"\n",
    "    Enhanced content type classification using keywords and source information.\n",
    "    Returns both main category and subcategory.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'unknown', 'unknown'\n",
    "    \n",
    "    # Convert to lowercase for better matching\n",
    "    text_lower = text.lower()\n",
    "    words = set(word_tokenize(text_lower))\n",
    "    \n",
    "    # Define category keywords\n",
    "    categories = {\n",
    "        'legal': {\n",
    "            'keywords': {'court', 'law', 'judge', 'legal', 'plaintiff', 'defendant', 'ruling', 'verdict', 'opinion', 'order', 'case'},\n",
    "            'subcategories': {\n",
    "                'court_decision': {'opinion', 'order', 'court', 'ruling', 'judgment'},\n",
    "                'legal_document': {'brief', 'motion', 'petition', 'filing'},\n",
    "                'legal_analysis': {'analysis', 'commentary', 'review'}\n",
    "            }\n",
    "        },\n",
    "        'science': {\n",
    "            'keywords': {'research', 'study', 'experiment', 'scientific', 'analysis', 'data', 'methodology', 'results'},\n",
    "            'subcategories': {\n",
    "                'medical': {'patient', 'treatment', 'clinical', 'medical', 'health', 'disease'},\n",
    "                'technology': {'software', 'computer', 'algorithm', 'system', 'technology', 'digital'},\n",
    "                'environmental': {'climate', 'environmental', 'ecology', 'sustainable', 'conservation'},\n",
    "                'physics': {'physics', 'quantum', 'particle', 'energy', 'matter'},\n",
    "                'biology': {'biology', 'cell', 'gene', 'protein', 'organism'}\n",
    "            }\n",
    "        },\n",
    "        'history': {\n",
    "            'keywords': {'history', 'historical', 'century', 'ancient', 'era', 'period', 'war', 'revolution'},\n",
    "            'subcategories': {\n",
    "                'political_history': {'government', 'politics', 'revolution', 'regime', 'democracy'},\n",
    "                'social_history': {'society', 'cultural', 'social', 'community'},\n",
    "                'military_history': {'war', 'battle', 'military', 'army', 'conflict'},\n",
    "                'economic_history': {'economy', 'trade', 'market', 'economic', 'financial'}\n",
    "            }\n",
    "        },\n",
    "        'academic': {\n",
    "            'keywords': {'university', 'academic', 'research', 'study', 'theory', 'analysis'},\n",
    "            'subcategories': {\n",
    "                'research_paper': {'methodology', 'findings', 'conclusion', 'abstract'},\n",
    "                'thesis': {'dissertation', 'thesis', 'research', 'study'},\n",
    "                'review': {'literature', 'review', 'analysis', 'critique'}\n",
    "            }\n",
    "        },\n",
    "        'news': {\n",
    "            'keywords': {'news', 'report', 'journalist', 'press', 'media', 'announcement'},\n",
    "            'subcategories': {\n",
    "                'current_events': {'current', 'today', 'recent', 'latest'},\n",
    "                'press_release': {'announces', 'released', 'statement', 'press'},\n",
    "                'news_article': {'reported', 'according', 'sources'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Source-based initial classification\n",
    "    if source in ['courtlistener', 'Caselaw_Access_Project']:\n",
    "        main_category = 'legal'\n",
    "    elif source == 'Wikipedia':\n",
    "        # For Wikipedia, we'll determine category based on content\n",
    "        main_category = None\n",
    "    else:\n",
    "        main_category = None\n",
    "    \n",
    "    # If main_category wasn't determined by source, use content analysis\n",
    "    if not main_category:\n",
    "        # Count keyword matches for each category\n",
    "        category_scores = {}\n",
    "        for category, data in categories.items():\n",
    "            keyword_matches = len(words.intersection(data['keywords']))\n",
    "            category_scores[category] = keyword_matches\n",
    "        \n",
    "        # Get category with highest score\n",
    "        if any(category_scores.values()):\n",
    "            main_category = max(category_scores.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            main_category = 'general'\n",
    "    \n",
    "    # Determine subcategory\n",
    "    if main_category in categories:\n",
    "        subcategory_scores = {}\n",
    "        for subcategory, subcategory_keywords in categories[main_category]['subcategories'].items():\n",
    "            matches = len(words.intersection(subcategory_keywords))\n",
    "            subcategory_scores[subcategory] = matches\n",
    "        \n",
    "        if any(subcategory_scores.values()):\n",
    "            subcategory = max(subcategory_scores.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            subcategory = f'{main_category}_general'\n",
    "    else:\n",
    "        subcategory = 'general'\n",
    "    \n",
    "    return main_category, subcategory\n",
    "\n",
    "def enhance_content_classification(df):\n",
    "    \"\"\"\n",
    "    Enhance the content classification of the dataset.\n",
    "    \"\"\"\n",
    "    # Create new columns for main category and subcategory\n",
    "    df[['main_category', 'subcategory']] = pd.DataFrame(\n",
    "        df.apply(lambda x: get_detailed_content_type(x['text'], x['collection']), axis=1).tolist(),\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # Additional quality metrics\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['sentence_count'] = df['text'].apply(lambda x: len(sent_tokenize(x)) if pd.notna(x) else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "enhanced_df = enhance_content_classification(further_cleaned_df)\n",
    "\n",
    "# Get classification statistics\n",
    "def get_classification_stats(df):\n",
    "    stats = {\n",
    "        'main_category_distribution': df['main_category'].value_counts(),\n",
    "        'subcategory_distribution': df['subcategory'].value_counts(),\n",
    "        'avg_text_length_by_category': df.groupby('main_category')['text_length'].mean(),\n",
    "        'avg_sentences_by_category': df.groupby('main_category')['sentence_count'].mean(),\n",
    "        'source_category_correlation': pd.crosstab(df['collection'], df['main_category'])\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "stats = get_classification_stats(enhanced_df)\n",
    "print(\"Classification Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "587566ec-c3b0-40e3-b7c6-51136b187bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>collection</th>\n",
       "      <th>text</th>\n",
       "      <th>is_english</th>\n",
       "      <th>content_type</th>\n",
       "      <th>main_category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>text_length</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-3607956D-A_1</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Process preparing allyl chloride methallyl chl...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>17290</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sn84022149_1878-07-25_1_2_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>Daily Leader . rmmm daily , xxcxrr Monday , Tn...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>20179</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sermons01blai_1_9</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>defire difeovers liberal mind , con- netted ma...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>24575</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://eunis.eea.europa.eu/species/224803</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Kingdom Animalia Phylum Arthropoda Class Arach...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>1307</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.perseus.tufts.edu/hopper/text?doc=u...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>23 faith , Moses , born , hidden three month p...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>history</td>\n",
       "      <td>history_general</td>\n",
       "      <td>751</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20621</th>\n",
       "      <td>https://blog.geoactivegroup.com/2018/11/connec...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Technology Media Telecommunications Tuesday , ...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>science</td>\n",
       "      <td>technology</td>\n",
       "      <td>2245</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20622</th>\n",
       "      <td>1108673_2003_2</td>\n",
       "      <td>SEC</td>\n",
       "      <td>Gain Sale Discontinued Operations . year ended...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>33118</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20623</th>\n",
       "      <td>collegeofpharmac1959coll_1</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>academic</td>\n",
       "      <td>thesis</td>\n",
       "      <td>42071</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20624</th>\n",
       "      <td>https://bibtex.github.io/person/Yongseok_Oh.html</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Travelled 1 Spain Collaborated E.Lee D.Lee Tal...</td>\n",
       "      <td>True</td>\n",
       "      <td>short_form</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>487</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20625</th>\n",
       "      <td>US-78283607-A_3</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Plasmid pLR186 derived commercially available ...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>science</td>\n",
       "      <td>biology</td>\n",
       "      <td>24894</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19147 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              identifier  \\\n",
       "0                                        US-3607956D-A_1   \n",
       "1                            sn84022149_1878-07-25_1_2_1   \n",
       "2                                      sermons01blai_1_9   \n",
       "3              http://eunis.eea.europa.eu/species/224803   \n",
       "4      http://www.perseus.tufts.edu/hopper/text?doc=u...   \n",
       "...                                                  ...   \n",
       "20621  https://blog.geoactivegroup.com/2018/11/connec...   \n",
       "20622                                     1108673_2003_2   \n",
       "20623                         collegeofpharmac1959coll_1   \n",
       "20624   https://bibtex.github.io/person/Yongseok_Oh.html   \n",
       "20625                                    US-78283607-A_3   \n",
       "\n",
       "                          collection  \\\n",
       "0                              USPTO   \n",
       "1                   US-PD-Newspapers   \n",
       "2                         English-PD   \n",
       "3      Creative Commons Common Crawl   \n",
       "4      Creative Commons Common Crawl   \n",
       "...                              ...   \n",
       "20621  Creative Commons Common Crawl   \n",
       "20622                            SEC   \n",
       "20623                     English-PD   \n",
       "20624  Creative Commons Common Crawl   \n",
       "20625                          USPTO   \n",
       "\n",
       "                                                    text  is_english  \\\n",
       "0      Process preparing allyl chloride methallyl chl...        True   \n",
       "1      Daily Leader . rmmm daily , xxcxrr Monday , Tn...        True   \n",
       "2      defire difeovers liberal mind , con- netted ma...        True   \n",
       "3      Kingdom Animalia Phylum Arthropoda Class Arach...        True   \n",
       "4      23 faith , Moses , born , hidden three month p...        True   \n",
       "...                                                  ...         ...   \n",
       "20621  Technology Media Telecommunications Tuesday , ...        True   \n",
       "20622  Gain Sale Discontinued Operations . year ended...        True   \n",
       "20623  COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...        True   \n",
       "20624  Travelled 1 Spain Collaborated E.Lee D.Lee Tal...        True   \n",
       "20625  Plasmid pLR186 derived commercially available ...        True   \n",
       "\n",
       "      content_type main_category      subcategory  text_length  sentence_count  \n",
       "0          general         legal   court_decision        17290             132  \n",
       "1          general         legal   court_decision        20179             307  \n",
       "2          general         legal   court_decision        24575             302  \n",
       "3          general         legal   court_decision         1307              10  \n",
       "4          general       history  history_general          751               7  \n",
       "...            ...           ...              ...          ...             ...  \n",
       "20621      general       science       technology         2245              17  \n",
       "20622      general         legal   court_decision        33118             228  \n",
       "20623      general      academic           thesis        42071             602  \n",
       "20624   short_form       general          general          487               4  \n",
       "20625      general       science          biology        24894             212  \n",
       "\n",
       "[19147 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497c0863-d82d-42e0-a48c-15c4f30e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_df = enhanced_df.drop(columns=['content_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c34e4b88-fed9-4ecf-8e44-bd865a84960a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>collection</th>\n",
       "      <th>text</th>\n",
       "      <th>is_english</th>\n",
       "      <th>main_category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>text_length</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-3607956D-A_1</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Process preparing allyl chloride methallyl chl...</td>\n",
       "      <td>True</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>17290</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sn84022149_1878-07-25_1_2_1</td>\n",
       "      <td>US-PD-Newspapers</td>\n",
       "      <td>Daily Leader . rmmm daily , xxcxrr Monday , Tn...</td>\n",
       "      <td>True</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>20179</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sermons01blai_1_9</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>defire difeovers liberal mind , con- netted ma...</td>\n",
       "      <td>True</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>24575</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://eunis.eea.europa.eu/species/224803</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Kingdom Animalia Phylum Arthropoda Class Arach...</td>\n",
       "      <td>True</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>1307</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.perseus.tufts.edu/hopper/text?doc=u...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>23 faith , Moses , born , hidden three month p...</td>\n",
       "      <td>True</td>\n",
       "      <td>history</td>\n",
       "      <td>history_general</td>\n",
       "      <td>751</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20621</th>\n",
       "      <td>https://blog.geoactivegroup.com/2018/11/connec...</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Technology Media Telecommunications Tuesday , ...</td>\n",
       "      <td>True</td>\n",
       "      <td>science</td>\n",
       "      <td>technology</td>\n",
       "      <td>2245</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20622</th>\n",
       "      <td>1108673_2003_2</td>\n",
       "      <td>SEC</td>\n",
       "      <td>Gain Sale Discontinued Operations . year ended...</td>\n",
       "      <td>True</td>\n",
       "      <td>legal</td>\n",
       "      <td>court_decision</td>\n",
       "      <td>33118</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20623</th>\n",
       "      <td>collegeofpharmac1959coll_1</td>\n",
       "      <td>English-PD</td>\n",
       "      <td>COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...</td>\n",
       "      <td>True</td>\n",
       "      <td>academic</td>\n",
       "      <td>thesis</td>\n",
       "      <td>42071</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20624</th>\n",
       "      <td>https://bibtex.github.io/person/Yongseok_Oh.html</td>\n",
       "      <td>Creative Commons Common Crawl</td>\n",
       "      <td>Travelled 1 Spain Collaborated E.Lee D.Lee Tal...</td>\n",
       "      <td>True</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>487</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20625</th>\n",
       "      <td>US-78283607-A_3</td>\n",
       "      <td>USPTO</td>\n",
       "      <td>Plasmid pLR186 derived commercially available ...</td>\n",
       "      <td>True</td>\n",
       "      <td>science</td>\n",
       "      <td>biology</td>\n",
       "      <td>24894</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19147 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              identifier  \\\n",
       "0                                        US-3607956D-A_1   \n",
       "1                            sn84022149_1878-07-25_1_2_1   \n",
       "2                                      sermons01blai_1_9   \n",
       "3              http://eunis.eea.europa.eu/species/224803   \n",
       "4      http://www.perseus.tufts.edu/hopper/text?doc=u...   \n",
       "...                                                  ...   \n",
       "20621  https://blog.geoactivegroup.com/2018/11/connec...   \n",
       "20622                                     1108673_2003_2   \n",
       "20623                         collegeofpharmac1959coll_1   \n",
       "20624   https://bibtex.github.io/person/Yongseok_Oh.html   \n",
       "20625                                    US-78283607-A_3   \n",
       "\n",
       "                          collection  \\\n",
       "0                              USPTO   \n",
       "1                   US-PD-Newspapers   \n",
       "2                         English-PD   \n",
       "3      Creative Commons Common Crawl   \n",
       "4      Creative Commons Common Crawl   \n",
       "...                              ...   \n",
       "20621  Creative Commons Common Crawl   \n",
       "20622                            SEC   \n",
       "20623                     English-PD   \n",
       "20624  Creative Commons Common Crawl   \n",
       "20625                          USPTO   \n",
       "\n",
       "                                                    text  is_english  \\\n",
       "0      Process preparing allyl chloride methallyl chl...        True   \n",
       "1      Daily Leader . rmmm daily , xxcxrr Monday , Tn...        True   \n",
       "2      defire difeovers liberal mind , con- netted ma...        True   \n",
       "3      Kingdom Animalia Phylum Arthropoda Class Arach...        True   \n",
       "4      23 faith , Moses , born , hidden three month p...        True   \n",
       "...                                                  ...         ...   \n",
       "20621  Technology Media Telecommunications Tuesday , ...        True   \n",
       "20622  Gain Sale Discontinued Operations . year ended...        True   \n",
       "20623  COLUMBIA UNIVERSITY BULLETIN Fifty-ninth Serie...        True   \n",
       "20624  Travelled 1 Spain Collaborated E.Lee D.Lee Tal...        True   \n",
       "20625  Plasmid pLR186 derived commercially available ...        True   \n",
       "\n",
       "      main_category      subcategory  text_length  sentence_count  \n",
       "0             legal   court_decision        17290             132  \n",
       "1             legal   court_decision        20179             307  \n",
       "2             legal   court_decision        24575             302  \n",
       "3             legal   court_decision         1307              10  \n",
       "4           history  history_general          751               7  \n",
       "...             ...              ...          ...             ...  \n",
       "20621       science       technology         2245              17  \n",
       "20622         legal   court_decision        33118             228  \n",
       "20623      academic           thesis        42071             602  \n",
       "20624       general          general          487               4  \n",
       "20625       science          biology        24894             212  \n",
       "\n",
       "[19147 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602b7158-047b-4fbe-bdd0-9b6ad38192fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the 'text' column: 124105982\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Apply the function to the 'text' column and sum the results\n",
    "total_tokens = df['text'].apply(count_tokens).sum()\n",
    "\n",
    "print(f\"Total number of tokens in the 'text' column: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819e0c97-9dcf-4dd8-aec1-196cb2e73915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the 'text' column: 39527395\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Apply the function to the 'text' column and sum the results\n",
    "total_tokens = enhanced_df['text'].apply(count_tokens).sum()\n",
    "\n",
    "print(f\"Total number of tokens in the 'text' column: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdc33cf7-1e8a-473b-8e9d-45c69c1a42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save processed dataset\n",
    "enhanced_df.to_parquet('processed_dataset2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdf4e2-14d8-4fc6-a5c7-2ea6b04d0135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
